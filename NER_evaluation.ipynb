{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER-evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmrf/NER-evaluation/blob/master/NER_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naMwpdZZPutv",
        "colab_type": "text"
      },
      "source": [
        "# NER evaluation and comparison\n",
        "\n",
        "We will compare 3 different libraries with their pre-trained models for Name Entity Extraction (NER)\n",
        "\n",
        "* [Spacy](https://github.com/explosion/spaCy)\n",
        "* [Deeppavlov](https://github.com/deepmipt/DeepPavlov)\n",
        "* [Polyglot](https://github.com/aboSamoor/polyglot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Eye3WlCeskK6"
      },
      "source": [
        "### Grant acces to Drive \n",
        "\n",
        "If we don't grant acces to drive, **we will lose the generated data** every time we restart the colab runtime"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KElXr-QzscHb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "6625c480-9c36-4e31-f226-11956cfc0c6c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVPNLxmaPX8U",
        "colab_type": "text"
      },
      "source": [
        "### Install all needed libraries and download the pre-trained models\n",
        "\n",
        "There's quite a bit to download and to compile... so go and grab a coffee in the meantime ;)\n",
        "\n",
        "**IMPORTANT**: After the installation process is complete you'll need to **_restart the collab runtime_** so the changes and installed models take effect!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9Z19HihOvhh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install spacy\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_lg\n",
        "\n",
        "# install deepPavlov\n",
        "!pip install deeppavlov\n",
        "!python -m deeppavlov install squad_bert\n",
        "!python -m deeppavlov install ner_ontonotes_bert\n",
        "\n",
        "# install Polyglot\n",
        "!pip install polyglot\n",
        "!pip install pyicu\n",
        "!pip install pycld2\n",
        "!pip install morfessor\n",
        "!polyglot download embeddings2.en ner2.en"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-3JGicuPO-C",
        "colab_type": "text"
      },
      "source": [
        "### Define some helper classes to perform the NER extraction\n",
        "\n",
        "We also define a mapping between the different tagging conventions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DfUW1ROPA9P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "\n",
        "from polyglot.text import Text\n",
        "from deeppavlov import configs, build_model\n",
        "\n",
        "\n",
        "polyglot2spacy = {\n",
        "    \"I-LOC\":\"GPE\",\n",
        "    \"I-ORG\":\"ORG\",\n",
        "    \"I-PER\":\"PERSON\"\n",
        "}\n",
        "\n",
        "pavlov2spacy = {\n",
        "    \"LOCATION\":\"LOC\",\n",
        "    \"ORGANIZATION\":\"ORG\",\n",
        "    \"PERSON\":\"PERSON\"\n",
        "}\n",
        "\n",
        "\n",
        "class PolyglotNER():\n",
        "\n",
        "    @classmethod\n",
        "    def process(cls, text):\n",
        "        doc = Text(text)\n",
        "        return dict([(t._collection[0], polyglot2spacy.get(t.tag, t.tag))\n",
        "                     for t in doc.entities])\n",
        "\n",
        "\n",
        "class PavlovNER():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.ner_model = build_model(\n",
        "            configs.ner.ner_ontonotes_bert_mult, \n",
        "            download=True\n",
        "        )\n",
        "\n",
        "    def process(self, text):\n",
        "        sents, bios = self.ner_model([text])\n",
        "        # return a dict mapping entity to type\n",
        "        return dict([(w, tag.split(\"-\")[-1])\n",
        "                    for w, tag in zip(sents[0], bios[0]) \n",
        "                    if tag != 'O'])\n",
        "\n",
        "\n",
        "class SpacyNER():\n",
        "\n",
        "    def __init__(self):\n",
        "        # load model\n",
        "        self.nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "    def process(self, text):\n",
        "        doc = self.nlp(text)\n",
        "        # return a dict mapping entity to type\n",
        "        return dict([(e.text, e.ent_type_)\n",
        "                     for e in doc if e.ent_type != 0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CU38OQpVfTv",
        "colab_type": "text"
      },
      "source": [
        "### Load the different models\n",
        "\n",
        "We do this only once. DeepPavlov should be called SlowPavlov... :/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qx770uLKPh2s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "95535908-6635-4cb4-f0ef-83bbf6f88951"
      },
      "source": [
        "spacy_ner = SpacyNER()\n",
        "pavlov_ner = PavlovNER()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-09-26 13:56:27.749 INFO in 'deeppavlov.core.data.utils'['utils'] at line 64: Downloading from http://files.deeppavlov.ai/deeppavlov_data/ner_ontonotes_bert_mult_v1.tar.gz to /root/.deeppavlov/ner_ontonotes_bert_mult_v1.tar.gz\n",
            "100%|██████████| 1.32G/1.32G [06:02<00:00, 3.64MB/s]\n",
            "2019-09-26 14:02:29.779 INFO in 'deeppavlov.core.data.utils'['utils'] at line 216: Extracting /root/.deeppavlov/ner_ontonotes_bert_mult_v1.tar.gz archive into /root/.deeppavlov/models\n",
            "2019-09-26 14:02:47.49 INFO in 'deeppavlov.core.data.utils'['utils'] at line 64: Downloading from http://files.deeppavlov.ai/deeppavlov_data/bert/multi_cased_L-12_H-768_A-12.zip to /root/.deeppavlov/downloads/multi_cased_L-12_H-768_A-12.zip\n",
            " 80%|███████▉  | 527M/663M [01:24<00:25, 5.36MB/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yZ2iY-mPfTE",
        "colab_type": "text"
      },
      "source": [
        "### Run some prints to understand each library outputs\n",
        "\n",
        "Once the models are loaded into memory is a just a matter of running the inputs through"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iu_7xP2tQ4Jj",
        "colab_type": "code",
        "outputId": "c656e4d5-30a6-4860-905c-0bace47ba584",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        }
      },
      "source": [
        "# sample sentence:\n",
        "text = \"This is Marcos from the Golden Bridge of Barcelona saying hello to all of Spain!\"\n",
        "\n",
        "# Spacy\n",
        "print(\"Spacy: {}\\n\".format(spacy_ner.process(text)))\n",
        "# Polyglot\n",
        "print(\"Polyglot: {}\\n\".format(PolyglotNER.process(text)))\n",
        "# Pavlov\n",
        "print(\"Pavlov: {}\\n\".format(pavlov_ner.process(text)))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spacy: {'Marcos': 'PERSON', 'the': 'FAC', 'Golden': 'FAC', 'Bridge': 'FAC', 'Barcelona': 'GPE', 'Spain': 'GPE'}\n",
            "\n",
            "Polyglot: {'Marcos': 'PERSON', 'Barcelona': 'GPE', 'Spain': 'GPE'}\n",
            "\n",
            "Pavlov: {'Marcos': 'PERSON', 'the': 'FAC', 'Golden': 'FAC', 'Bridge': 'FAC', 'Barcelona': 'GPE', 'Spain': 'GPE'}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtgQzOav4le_",
        "colab_type": "text"
      },
      "source": [
        "### Sets difference and intersection example\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7qppt48neZO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "a67e4278-7b67-475a-da14-c4c6ed798d46"
      },
      "source": [
        "GT = {\"Africa\",\"Australia\"}\n",
        "ner = {\"Africa\",\"ASDs\"}\n",
        "\n",
        "print(GT-ner,len(list(GT-ner)))\n",
        "print(ner-GT,len(list(ner-GT)))\n",
        "print(GT.intersection(ner),len(list(GT.intersection(ner))))\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Australia'} 1\n",
            "{'ASDs'} 1\n",
            "{'Africa'} 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHfIiyK8WOJt",
        "colab_type": "text"
      },
      "source": [
        "### Load the corpus of tagged sentences with entities\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnMS0rJgV-_j",
        "colab_type": "code",
        "outputId": "0f1c32aa-a01f-4039-9da5-7f20ac3298c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        }
      },
      "source": [
        "from collections import defaultdict\n",
        "import ast\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "This function provide the output following the next JSON/dict format:\n",
        "{'Is Africa in your catalogue': \n",
        "    {'GT': \n",
        "          {'Africa': 'LOC'}\n",
        "    }\n",
        "}\n",
        "\"\"\"\n",
        "def load_data(path):\n",
        "    #Read file\n",
        "    with open(path) as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    #Interpret file and convert to lists\n",
        "    texts = []\n",
        "    tokens = []\n",
        "    ent_type = [] \n",
        "    for line in lines:\n",
        "        line = ast.literal_eval(line)\n",
        "        texts.append(line[0])\n",
        "        tokens.append(line[1])\n",
        "        ent_type.append(line[2])\n",
        "\n",
        "    #Translate from lists to the desired format\n",
        "    data = {}\n",
        "    for token,ent,txt in zip(tokens,ent_type,texts):    \n",
        "        data[txt]={\"GT\":{}}\n",
        "        for i,en in enumerate(ent):\n",
        "            if en!=\"O\":\n",
        "                data[txt][\"GT\"][token[i]]=en\n",
        "    return data\n",
        "\n",
        "\n",
        "# def get_entities(data: dict) -> Dict[Text[Dict[Text,Text]]]:\n",
        "def get_entities(corpus):\n",
        "    entity_results = {}\n",
        "    for text in corpus:\n",
        "        # entity_results[text]={}\n",
        "        entity_results[text]=corpus[text]\n",
        "        entity_results[text][\"Spacy\"]=spacy_ner.process(text)\n",
        "        entity_results[text][\"Polyglot\"]=PolyglotNER.process(text)\n",
        "        entity_results[text][\"Pavlov\"]=pavlov_ner.process(text)\n",
        "    return entity_results\n",
        "\n",
        "def evaluate_entities(entities):\n",
        "  ner_extractors = [\"Spacy\",\"Polyglot\",\"Pavlov\"]\n",
        "  results = {}\n",
        "  for extractor in ner_extractors:\n",
        "    results[extractor] = {\"TP\":0,\"TN\":0,\"FP\":0,\"FN\":0,\"Acc\":0}\n",
        "    for sentence in entities:\n",
        "      gt = set(entities[sentence][\"GT\"].keys())\n",
        "      extractor_result = set(entities[sentence][extractor].keys())\n",
        "      results[extractor][\"TP\"] += len(list(gt.intersection(extractor_result)))\n",
        "      results[extractor][\"FP\"] += len(list(extractor_result-gt))\n",
        "      results[extractor][\"FN\"] += len(list(gt-extractor_result))\n",
        "    \n",
        "    tp = results[extractor][\"TP\"]\n",
        "    tn = results[extractor][\"TN\"]\n",
        "    fp = results[extractor][\"FP\"]\n",
        "    fn = results[extractor][\"FN\"]\n",
        "    results[extractor][\"Acc\"]= tp / (tp+tn+fp+fn)\n",
        "\n",
        "  return results\n",
        "\n",
        "\n",
        "# load the entity corpus and\n",
        "corpus = load_data('/content/drive/My Drive/expanded.txt')\n",
        "\n",
        "# get the entities from all aviable extractors\n",
        "entities = get_entities(corpus)\n",
        "print(entities)\n",
        "\n",
        "# evaluate each extractor\n",
        "results = evaluate_entities(entities)\n",
        "print(results)\n",
        "\n",
        "\n",
        "with open(\"/content/drive/My Drive/entities.json\", \"w\") as file:\n",
        "    json.dump(entities, file, indent=4)\n",
        "\n",
        "with open(\"/content/drive/My Drive/results.json\", \"w\") as file:\n",
        "    json.dump(results, file, indent=4)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-376f89ab7e17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;31m# get the entities from all aviable extractors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m \u001b[0mentities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_entities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-376f89ab7e17>\u001b[0m in \u001b[0;36mget_entities\u001b[0;34m(corpus)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mentity_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mentity_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Spacy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspacy_ner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mentity_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Polyglot\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPolyglotNER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mentity_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Pavlov\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpavlov_ner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mentity_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-4de186c372d1>\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(cls, text)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         return dict([(t._collection[0], polyglot2spacy.get(t.tag, t.tag))\n\u001b[0;32m---> 26\u001b[0;31m                      for t in doc.entities])\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/polyglot/decorators.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, cls)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/polyglot/text.py\u001b[0m in \u001b[0;36mentities\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0mprev_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mu'O'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mne_chunker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mprev_tag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprev_tag\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mu'O'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/polyglot/decorators.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, cls)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/polyglot/text.py\u001b[0m in \u001b[0;36mne_chunker\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mcached_property\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mne_chunker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_ner_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mcached_property\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/polyglot/decorators.py\u001b[0m in \u001b[0;36mmemoizer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m       \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmemoizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/polyglot/tag/base.py\u001b[0m in \u001b[0;36mget_ner_tagger\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_ner_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m   \u001b[0;34m\"\"\"Return a NER tagger from the models cache.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mNEChunker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/polyglot/tag/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mlang\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[0mcode\u001b[0m \u001b[0mto\u001b[0m \u001b[0mdecide\u001b[0m \u001b[0mwhich\u001b[0m \u001b[0mchunker\u001b[0m \u001b[0mto\u001b[0m \u001b[0muse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \"\"\"\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNEChunker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mID_TAG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNER_ID_TAG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/polyglot/tag/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \"\"\"\n\u001b[1;32m     39\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mID_TAG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/polyglot/tag/base.py\u001b[0m in \u001b[0;36m_load_network\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_load_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;34m\"\"\" Building the predictor out of the model.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cw'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_ner_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mfirst_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/polyglot/decorators.py\u001b[0m in \u001b[0;36mmemoizer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m       \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmemoizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/polyglot/load.py\u001b[0m in \u001b[0;36mload_embeddings\u001b[0;34m(lang, task, type, normalize)\u001b[0m\n\u001b[1;32m     59\u001b[0m   \"\"\"\n\u001b[1;32m     60\u001b[0m   \u001b[0msrc_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"_\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m   \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocate_resource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cw\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/polyglot/load.py\u001b[0m in \u001b[0;36mlocate_resource\u001b[0;34m(name, lang, filter)\u001b[0m\n\u001b[1;32m     41\u001b[0m   \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolyglot_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mdownloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdownloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINSTALLED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m       raise ValueError(\"This resource is available in the index \"\n\u001b[1;32m     45\u001b[0m                        \u001b[0;34m\"but not downloaded, yet. Try to run\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/polyglot/downloader.py\u001b[0m in \u001b[0;36mstatus\u001b[0;34m(self, info_or_id, download_dir)\u001b[0m\n\u001b[1;32m    735\u001b[0m     \"\"\"\n\u001b[1;32m    736\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdownload_dir\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdownload_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m     \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_or_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_or_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m     \u001b[0;31m# Handle collections:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/polyglot/downloader.py\u001b[0m in \u001b[0;36m_info_or_id\u001b[0;34m(self, info_or_id)\u001b[0m\n\u001b[1;32m    505\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_info_or_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo_or_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_or_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_or_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0minfo_or_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/polyglot/downloader.py\u001b[0m in \u001b[0;36minfo\u001b[0;34m(self, id)\u001b[0m\n\u001b[1;32m    931\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_packages\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_packages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_collections\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_collections\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Package %r not found in index'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Package 'embeddings2.gn' not found in index"
          ]
        }
      ]
    }
  ]
}