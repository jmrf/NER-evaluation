{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER-evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmrf/NER-evaluation/blob/master/NER_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naMwpdZZPutv",
        "colab_type": "text"
      },
      "source": [
        "# NER evaluation and comparison\n",
        "\n",
        "We will compare 3 different libraries with their pre-trained models for Name Entity Extraction (NER)\n",
        "\n",
        "* [Spacy](https://github.com/explosion/spaCy)\n",
        "* [Deeppavlov](https://github.com/deepmipt/DeepPavlov)\n",
        "* [Polyglot](https://github.com/aboSamoor/polyglot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Eye3WlCeskK6"
      },
      "source": [
        "### Grant acces to Drive \n",
        "\n",
        "If we don't grant acces to drive, **we will lose the generated data** every time we restart the colab runtime"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KElXr-QzscHb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVPNLxmaPX8U",
        "colab_type": "text"
      },
      "source": [
        "### Install all needed libraries and download the pre-trained models\n",
        "\n",
        "There's quite a bit to download and to compile... so go and grab a coffee in the meantime ;)\n",
        "\n",
        "**IMPORTANT**: After the installation process is complete you'll need to **_restart the collab runtime_** so the changes and installed models take effect!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9Z19HihOvhh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install spacy\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_lg\n",
        "\n",
        "# install deepPavlov\n",
        "!pip install deeppavlov\n",
        "!python -m deeppavlov install squad_bert\n",
        "!python -m deeppavlov install ner_ontonotes_bert\n",
        "\n",
        "# install Polyglot\n",
        "!pip install polyglot\n",
        "!pip install pyicu\n",
        "!pip install pycld2\n",
        "!pip install morfessor\n",
        "!polyglot download embeddings2.en ner2.en"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-3JGicuPO-C",
        "colab_type": "text"
      },
      "source": [
        "### Define some helper classes to perform the NER extraction\n",
        "\n",
        "We also define a mapping between the different tagging conventions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DfUW1ROPA9P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "\n",
        "from polyglot.text import Text\n",
        "from deeppavlov import configs, build_model\n",
        "\n",
        "\n",
        "polyglot2spacy = {\n",
        "    \"I-LOC\":\"GPE\",\n",
        "    \"I-ORG\":\"ORG\",\n",
        "    \"I-PER\":\"PERSON\"\n",
        "}\n",
        "\n",
        "pavlov2spacy = {\n",
        "    \"LOCATION\":\"LOC\",\n",
        "    \"ORGANIZATION\":\"ORG\",\n",
        "    \"PERSON\":\"PERSON\"\n",
        "}\n",
        "\n",
        "\n",
        "class PolyglotNER():\n",
        "\n",
        "    @classmethod\n",
        "    def process(cls, text):\n",
        "        doc = Text(text)\n",
        "        return dict([(t._collection[0], polyglot2spacy.get(t.tag, t.tag))\n",
        "                     for t in doc.entities])\n",
        "\n",
        "\n",
        "class PavlovNER():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.ner_model = build_model(\n",
        "            configs.ner.ner_ontonotes_bert_mult, \n",
        "            download=True\n",
        "        )\n",
        "\n",
        "    def process(self, text):\n",
        "        sents, bios = self.ner_model([text])\n",
        "        # return a dict mapping entity to type\n",
        "        return dict([(w, tag.split(\"-\")[-1])\n",
        "                    for w, tag in zip(sents[0], bios[0]) \n",
        "                    if tag != 'O'])\n",
        "\n",
        "\n",
        "class SpacyNER():\n",
        "\n",
        "    def __init__(self):\n",
        "        # load model\n",
        "        self.nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "    def process(self, text):\n",
        "        doc = self.nlp(text)\n",
        "        # return a dict mapping entity to type\n",
        "        return dict([(e.text, e.ent_type_)\n",
        "                     for e in doc if e.ent_type != 0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CU38OQpVfTv",
        "colab_type": "text"
      },
      "source": [
        "### Load the different models\n",
        "\n",
        "We do this only once. DeepPavlov should be called SlowPavlov... :/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qx770uLKPh2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spacy_ner = SpacyNER()\n",
        "pavlov_ner = PavlovNER()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yZ2iY-mPfTE",
        "colab_type": "text"
      },
      "source": [
        "### Run some prints to understand each library outputs\n",
        "\n",
        "Once the models are loaded into memory is a just a matter of running the inputs through"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iu_7xP2tQ4Jj",
        "colab_type": "code",
        "outputId": "63620193-8f4f-4f77-bfab-723738e902ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        }
      },
      "source": [
        "# sample sentence:\n",
        "text = \"This is Marcos from the Golden Bridge of Barcelona saying hello to all of Spain!\"\n",
        "\n",
        "# Spacy\n",
        "print(\"Spacy: {}\\n\".format(spacy_ner.process(text)))\n",
        "# Polyglot\n",
        "print(\"Polyglot: {}\\n\".format(PolyglotNER.process(text)))\n",
        "# Pavlov\n",
        "print(\"Pavlov: {}\\n\".format(pavlov_ner.process(text)))\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spacy: {'Marcos': 'PERSON', 'the': 'FAC', 'Golden': 'FAC', 'Bridge': 'FAC', 'Barcelona': 'GPE', 'Spain': 'GPE'}\n",
            "\n",
            "Polyglot: {'Marcos': 'PERSON', 'Barcelona': 'GPE', 'Spain': 'GPE'}\n",
            "\n",
            "Pavlov: {'Marcos': 'PERSON', 'the': 'FAC', 'Golden': 'FAC', 'Bridge': 'FAC', 'Barcelona': 'GPE', 'Spain': 'GPE'}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtgQzOav4le_",
        "colab_type": "text"
      },
      "source": [
        "### Sets difference and intersection example\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7qppt48neZO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "36cd3cf1-575b-4764-aee1-112a2027c960"
      },
      "source": [
        "GT = {\"Africa\",\"Australia\"}\n",
        "ner = {\"Africa\",\"ASDs\"}\n",
        "\n",
        "print(GT-ner,len(list(GT-ner)))\n",
        "print(ner-GT,len(list(ner-GT)))\n",
        "print(GT.intersection(ner),len(list(GT.intersection(ner))))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Australia'} 1\n",
            "{'ASDs'} 1\n",
            "{'Africa'} 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHfIiyK8WOJt",
        "colab_type": "text"
      },
      "source": [
        "### Load the corpus of tagged sentences with entities\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnMS0rJgV-_j",
        "colab_type": "code",
        "outputId": "0921f16b-5cba-4b15-a07b-c8d095ca952d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        }
      },
      "source": [
        "from collections import defaultdict\n",
        "import ast\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "This function provide the output following the next JSON/dict format:\n",
        "{'Is Africa in your catalogue': \n",
        "    {'GT': \n",
        "          {'Africa': 'LOC'}\n",
        "    }\n",
        "}\n",
        "\"\"\"\n",
        "def load_data(path):\n",
        "    #Read file\n",
        "    with open(path) as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    #Interpret file and convert to lists\n",
        "    texts = []\n",
        "    tokens = []\n",
        "    ent_type = [] \n",
        "    for line in lines:\n",
        "        line = ast.literal_eval(line)\n",
        "        texts.append(line[0])\n",
        "        tokens.append(line[1])\n",
        "        ent_type.append(line[2])\n",
        "\n",
        "    #Translate from lists to the desired format\n",
        "    data = {}\n",
        "    for token,ent,txt in zip(tokens,ent_type,texts):    \n",
        "        data[txt]={\"GT\":{}}\n",
        "        for i,en in enumerate(ent):\n",
        "            if en!=\"O\":\n",
        "                data[txt][\"GT\"][token[i]]=en\n",
        "    return data\n",
        "\n",
        "\n",
        "# def get_entities(data: dict) -> Dict[Text[Dict[Text,Text]]]:\n",
        "def get_entities(corpus):\n",
        "    entity_results = {}\n",
        "    for text in corpus:\n",
        "        # entity_results[text]={}\n",
        "        entity_results[text]=corpus[text]\n",
        "        entity_results[text][\"Spacy\"]=spacy_ner.process(text)\n",
        "        entity_results[text][\"Polyglot\"]=PolyglotNER.process(text)\n",
        "        entity_results[text][\"Pavlov\"]=pavlov_ner.process(text)\n",
        "    return entity_results\n",
        "\n",
        "def evaluate_entities(entities):\n",
        "  ner_extractors = [\"Spacy\",\"Polyglot\",\"Pavlov\"]\n",
        "  results = {}\n",
        "  for extractor in ner_extractors:\n",
        "    results[extractor] = {\"TP\":0,\"TN\":0,\"FP\":0,\"FN\":0,\"Acc\":0}\n",
        "    for sentence in entities:\n",
        "      gt = set(entities[sentence][\"GT\"].keys())\n",
        "      extractor_result = set(entities[sentence][extractor].keys())\n",
        "      results[extractor][\"TP\"] += len(list(gt.intersection(extractor_result)))\n",
        "      results[extractor][\"FP\"] += len(list(extractor_result-gt))\n",
        "      results[extractor][\"FN\"] += len(list(gt-extractor_result))\n",
        "    \n",
        "    tp = results[extractor][\"TP\"]\n",
        "    tn = results[extractor][\"TN\"]\n",
        "    fp = results[extractor][\"FP\"]\n",
        "    fn = results[extractor][\"FN\"]\n",
        "    results[extractor][\"Acc\"]= tp / (tp+tn+fp+fn)\n",
        "\n",
        "  return results\n",
        "\n",
        "\n",
        "# load the entity corpus and\n",
        "corpus = load_data('/content/drive/My Drive/input_example.txt')\n",
        "\n",
        "# get the entities from all aviable extractors\n",
        "entities = get_entities(corpus)\n",
        "print(entities)\n",
        "\n",
        "# evaluate each extractor\n",
        "results = evaluate_entities(entities)\n",
        "print(results)\n",
        "\n",
        "\n",
        "with open(\"/content/drive/My Drive/entities.json\", \"w\") as file:\n",
        "    json.dump(entities, file, indent=4)\n",
        "\n",
        "with open(\"/content/drive/My Drive/results.json\", \"w\") as file:\n",
        "    json.dump(results, file, indent=4)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Is Africa in your catalogue': {'GT': {'Africa': 'LOC'}, 'Spacy': {'Africa': 'LOC'}, 'Polyglot': {'Africa': 'GPE'}, 'Pavlov': {'Africa': 'LOC'}}, 'Is Asia in your catalogue': {'GT': {'Asia': 'LOC'}, 'Spacy': {'Asia': 'LOC'}, 'Polyglot': {'Asia': 'GPE'}, 'Pavlov': {'Asia': 'LOC'}}, 'Is Aus in your catalogue': {'GT': {'Aus': 'LOC'}, 'Spacy': {}, 'Polyglot': {}, 'Pavlov': {}}, 'Is Australia in your catalogue': {'GT': {'Australia': 'LOC'}, 'Spacy': {'Australia': 'GPE'}, 'Polyglot': {'Australia': 'GPE'}, 'Pavlov': {'Australia': 'GPE'}}, 'Is Europe in your catalogue': {'GT': {'Europe': 'LOC'}, 'Spacy': {'Europe': 'LOC'}, 'Polyglot': {}, 'Pavlov': {'Europe': 'LOC'}}, 'Is Latin in your catalogue': {'GT': {'Latin': 'LOC'}, 'Spacy': {'Latin': 'NORP'}, 'Polyglot': {}, 'Pavlov': {'Latin': 'LANGUAGE'}}, 'Is North in your catalogue': {'GT': {'North': 'LOC'}, 'Spacy': {}, 'Polyglot': {}, 'Pavlov': {'North': 'LOC'}}, 'Is Costa Rica in your catalogue': {'GT': {'Costa': 'LOC', 'Rica': 'LOC'}, 'Spacy': {'Costa': 'GPE', 'Rica': 'GPE'}, 'Polyglot': {'Costa': 'GPE'}, 'Pavlov': {'Costa': 'GPE', 'Rica': 'GPE'}}}\n",
            "{'Spacy': {'TP': 7, 'TN': 0, 'FP': 0, 'FN': 2, 'Acc': 0.7777777777777778}, 'Polyglot': {'TP': 4, 'TN': 0, 'FP': 0, 'FN': 5, 'Acc': 0.4444444444444444}, 'Pavlov': {'TP': 8, 'TN': 0, 'FP': 0, 'FN': 1, 'Acc': 0.8888888888888888}}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}